{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7f75b62-0493-46da-a2b4-c4f8b1d27827",
   "metadata": {},
   "source": [
    "# The Power of Concurrency in Ollama: A Timed Comparison\n",
    "\n",
    "This notebook will practically demonstrate the speed and efficiency benefits of concurrent model serving in Ollama.\n",
    "\n",
    "\n",
    "We will time this entire workflow in two scenarios:\n",
    "* **Scenario A (Sequential):** Forcing a \"cold start\" for each step by loading and unloading each model.\n",
    "* **Scenario B (Concurrent):** Pre-loading all models into VRAM to make them \"hot\" and instantly available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75367dc9-fca6-49b1-a976-2a8bc88e1aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in e:\\anaconda\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: httpx>=0.27 in e:\\anaconda\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in e:\\anaconda\\lib\\site-packages (from ollama) (2.12.0)\n",
      "Requirement already satisfied: anyio in e:\\anaconda\\lib\\site-packages (from httpx>=0.27->ollama) (4.2.0)\n",
      "Requirement already satisfied: certifi in e:\\anaconda\\lib\\site-packages (from httpx>=0.27->ollama) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\anaconda\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in e:\\anaconda\\lib\\site-packages (from httpx>=0.27->ollama) (2.10)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\anaconda\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\anaconda\\lib\\site-packages (from pydantic>=2.9->ollama) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in e:\\anaconda\\lib\\site-packages (from pydantic>=2.9->ollama) (2.41.1)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in e:\\anaconda\\lib\\site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in e:\\anaconda\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\anaconda\\lib\\site-packages (from anyio->httpx>=0.27->ollama) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama\n",
    "import ollama\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Make sure you have these models pulled!\n",
    "# Run this in your terminal if you don't:\n",
    "\n",
    "Model1 = 'gemma2:2b'\n",
    "EMBED_MODEL = 'nomic-embed-text'\n",
    "USER_PROMPT = \"Write a short Python script to list all files in a directory.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e346e17-0955-4551-9fb6-2cb11b52b31f",
   "metadata": {},
   "source": [
    "## Scenario A: \"Sequential\" Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993e0d7a-6e5b-4247-b760-4c25e312db28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Classifying (Loading gemma2:2b)...\n",
      "Step 1 Complete (3.72s). Intent: ```python\n",
      "import os\n",
      "\n",
      "def list_files(directory):\n",
      "  \"\"\"Lists all files in the specified directory.\"\"\"\n",
      "  for filename in os.listdir(directory):\n",
      "    print(filename)\n",
      "\n",
      "# Example usage:\n",
      "list_files(\"/path/to/your/directory\") \n",
      "``` \n",
      "\n",
      "**Intent:** **coding** \n",
      "\n",
      "\n",
      "Step 2: Embedding (Loading nomic-embed-text)...\n",
      "Step 2 Complete (1.20s). Embedding dim: 768\n",
      "\n",
      "Step 3: Generating (Loading gemma2:2b)...\n",
      "Step 3 Complete (5.75s).\n",
      "------------------------------\n",
      "--- SCENARIO A TOTAL TIME: 10.67 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time_a = time.time()\n",
    "\n",
    "try:\n",
    "    # --- Step 1}: Classify (Cold Start) ---\n",
    "    print(f\"Step 1: Classifying (Loading {Model1})...\")\n",
    "    step1_start = time.time()\n",
    "    classify_response = ollama.chat(\n",
    "        model=Model1,\n",
    "        messages=[{\n",
    "            'role': 'system', \n",
    "            'content': \"Classify the user's intent as 'coding', 'billing', or 'general'. Respond with one word.\"\n",
    "        }, {\n",
    "            'role': 'user', \n",
    "            'content': USER_PROMPT\n",
    "        }] \n",
    "    )\n",
    "    print(f\"Step 1 Complete ({(time.time() - step1_start):.2f}s). Intent: {classify_response['message']['content']}\")\n",
    "\n",
    "    # --- Step 2: Embed (Cold Start) ---\n",
    "    print(f\"\\nStep 2: Embedding (Loading {EMBED_MODEL})...\")\n",
    "    step2_start = time.time()\n",
    "    embed_response = ollama.embeddings(\n",
    "        model=EMBED_MODEL,\n",
    "        prompt=USER_PROMPT\n",
    "    )\n",
    "    print(f\"Step 2 Complete ({(time.time() - step2_start):.2f}s). Embedding dim: {len(embed_response['embedding'])}\")\n",
    "\n",
    "    # --- Step 3: Generate (Cold Start) ---\n",
    "    print(f\"\\nStep 3: Generating (Loading {Model1})...\")\n",
    "    step3_start = time.time()\n",
    "    generate_response = ollama.chat(\n",
    "        model=Model1,\n",
    "        messages=[{'role': 'user', 'content': f\"Request: {USER_PROMPT}\"}],\n",
    "    )\n",
    "    print(f\"Step 3 Complete ({(time.time() - step3_start):.2f}s).\")\n",
    "    # print(f\"Response: {generate_response['message']['content'][:50]}...\")\n",
    "\n",
    "except ollama.ResponseError as e:\n",
    "    print(f\"Error: {e.error}\")\n",
    "    print(\"Please make sure you have pulled all three models: 'mistral:7b', 'nomic-embed-text', and 'llama3:8b'\")\n",
    "\n",
    "end_time_a = time.time()\n",
    "total_time_a = end_time_a - start_time_a\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"--- SCENARIO A TOTAL TIME: {total_time_a:.2f} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e172b9-7e69-4272-910e-34f2776147cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Classifying (Hot Run)...\n",
      "Step 1 Complete (1.55s). Intent: ```python\n",
      "import os\n",
      "\n",
      "def list_files(directory):\n",
      "  \"\"\"Lists all files in the specified directory.\"\"\"\n",
      "  for filename in os.listdir(directory):\n",
      "    print(filename) \n",
      "\n",
      "# Example usage:\n",
      "list_files('/path/to/your/directory') \n",
      "```\n",
      "\n",
      "\n",
      "**Classification:** **coding** \n",
      "\n",
      "\n",
      "Step 2: Embedding (Hot Run)...\n",
      "Step 2 Complete (0.06s). Embedding dim: 768\n",
      "\n",
      "Step 3: Generating (Hot Run)...\n",
      "Step 3 Complete (5.19s).\n",
      "------------------------------\n",
      "--- SCENARIO B TOTAL TIME: 6.80 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time_b = time.time()\n",
    "\n",
    "# --- Step 1: Classify (Hot Run) ---\n",
    "print(f\"Step 1: Classifying (Hot Run)...\")\n",
    "step1_start = time.time()\n",
    "classify_response = ollama.chat(\n",
    "    model=Model1,\n",
    "    messages=[{\n",
    "        'role': 'system', \n",
    "        'content': \"Classify the user's intent as 'coding', 'billing', or 'general'. Respond with one word.\"\n",
    "    }, {\n",
    "        'role': 'user', \n",
    "        'content': USER_PROMPT\n",
    "    }]\n",
    ")\n",
    "print(f\"Step 1 Complete ({(time.time() - step1_start):.2f}s). Intent: {classify_response['message']['content']}\")\n",
    "\n",
    "# --- Step 2: Embed (Hot Run) ---\n",
    "print(f\"\\nStep 2: Embedding (Hot Run)...\")\n",
    "step2_start = time.time()\n",
    "embed_response = ollama.embeddings(\n",
    "    model=EMBED_MODEL,\n",
    "    prompt=USER_PROMPT\n",
    ")\n",
    "print(f\"Step 2 Complete ({(time.time() - step2_start):.2f}s). Embedding dim: {len(embed_response['embedding'])}\")\n",
    "\n",
    "# --- Step 3: Generate (Hot Run) ---\n",
    "print(f\"\\nStep 3: Generating (Hot Run)...\")\n",
    "step3_start = time.time()\n",
    "generate_response = ollama.chat(\n",
    "    model=Model1,\n",
    "    messages=[{'role': 'user', 'content': f\"Request: {USER_PROMPT}\"}]\n",
    ")\n",
    "print(f\"Step 3 Complete ({(time.time() - step3_start):.2f}s).\")\n",
    "\n",
    "\n",
    "end_time_b = time.time()\n",
    "total_time_b = end_time_b - start_time_b\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"--- SCENARIO B TOTAL TIME: {total_time_b:.2f} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0447512-6a9f-445f-8a4d-02ed70070bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FINAL ANALYSIS ---\n",
      "Scenario A (Sequential Cold Starts) Time: 13.74 seconds\n",
      "Scenario B (Concurrent Hot Run) Time:    6.80 seconds\n",
      "------------------------------\n",
      "Concurrency saved 6.94 seconds.\n",
      "The concurrent workflow was 2.0x faster.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- FINAL ANALYSIS ---\")\n",
    "print(f\"Scenario A (Sequential Cold Starts) Time: {total_time_a:.2f} seconds\")\n",
    "print(f\"Scenario B (Concurrent Hot Run) Time:    {total_time_b:.2f} seconds\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if total_time_b > 0:\n",
    "    difference = total_time_a - total_time_b\n",
    "    performance_gain = (total_time_a / total_time_b)\n",
    "    print(f\"Concurrency saved {difference:.2f} seconds.\")\n",
    "    print(f\"The concurrent workflow was {performance_gain:.1f}x faster.\")\n",
    "else:\n",
    "    print(\"Scenario B was too fast to measure or an error occurred.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960d957-e52c-48ce-9cda-cdce31db6db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concurrency depends on Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf2f65-1281-4a3c-865a-d6577bc08ea2",
   "metadata": {},
   "source": [
    "# EXAMPLE -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78eabde4-cf7d-4836-b3c9-c90729a10c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " general\n",
      "11.206955671310425\n"
     ]
    }
   ],
   "source": [
    "start_time_1 = time.time()\n",
    "\n",
    "output = ollama.chat(\n",
    "    model='mistral:7b',\n",
    "    messages=[{\n",
    "        'role': 'system', \n",
    "        'content': \"Classify the user's intent as 'coding', 'billing', or 'general'. Respond with one word.\"\n",
    "    }, {\n",
    "        'role': 'user', \n",
    "        'content': \"What is photosynthesis\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(output['message']['content'])\n",
    "\n",
    "end_time_1= time.time()\n",
    "\n",
    "total_time_1 = end_time_1- start_time_1\n",
    "print(total_time_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ba56f5-b953-4613-be46-5fa1fdf637c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Coding\n",
      "0.481203556060791\n"
     ]
    }
   ],
   "source": [
    "start_time_2 = time.time()\n",
    "\n",
    "output = ollama.chat(\n",
    "    model='mistral:7b',\n",
    "    messages=[{\n",
    "        'role': 'system', \n",
    "        'content': \"Classify the user's intent as 'coding', 'billing', or 'general'. Respond with one word.\"\n",
    "    }, {\n",
    "        'role': 'user', \n",
    "        'content': \"What is GenAI\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(output['message']['content'])\n",
    "\n",
    "end_time_2= time.time()\n",
    "\n",
    "total_time_2 = end_time_2- start_time_2\n",
    "print(total_time_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b91d69b6-8d49-4e6c-8866-ca938ad2c1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FINAL ANALYSIS ---\n",
      "Scenario A (Sequential Cold Starts) Time: 11.21 seconds\n",
      "Scenario B (Concurrent Hot Run) Time:    0.48 seconds\n",
      "------------------------------\n",
      "Concurrency saved 10.73 seconds.\n",
      "The concurrent workflow was 23.3x faster.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- FINAL ANALYSIS ---\")\n",
    "print(f\"Scenario A (Sequential Cold Starts) Time: {total_time_1:.2f} seconds\")\n",
    "print(f\"Scenario B (Concurrent Hot Run) Time:    {total_time_2:.2f} seconds\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if total_time_2 > 0:\n",
    "    difference = total_time_1 - total_time_2\n",
    "    performance_gain = (total_time_1 / total_time_2)\n",
    "    print(f\"Concurrency saved {difference:.2f} seconds.\")\n",
    "    print(f\"The concurrent workflow was {performance_gain:.1f}x faster.\")\n",
    "else:\n",
    "    print(\"Scenario B was too fast to measure or an error occurred.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d156b43-4ca6-4fb4-9586-ce6afb96e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous models remvoed becasuse as per my working memory only 4gb model can be on my speed disk and other will be removed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2deca4b-7aae-4923-b6b4-1827fd5f407e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2fae2f-8d8d-473a-a4bd-eb023b1b1474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263ba91-075d-40f0-bb40-60fcfa66f6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d84f20-e69c-49ac-bc24-92545a99a41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf35f9-1d0f-421e-941c-3815405d3c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c4596e-c9c6-460f-ac5d-4ef76f5bd1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
